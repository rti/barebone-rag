---
title: Semantic Search and RAG on a FOSS stack
author: Robert Timm
---

<!-- footer: -->

![bg right](./slides_code_qrcode.png)

# Semantic Search and RAG on a FOSS stack

Wikimedia Hackathon Tallinn 2024

### [github.com/rti/barebone-rag](https://github.com/rti/barebone-rag)

Robert Timm <robert.timm@wikimedia.de>

---

<!-- paginate: true -->
<!-- header: Semantic Search and RAG on a FOSS stack -->

## Semantic Search

Given a query, find texts with a meaning similar.

## Retrieval Augmented Generation (RAG)

Create texts based on information loaded from external sources.

## Free and Open Source Software (FOSS) Stack

All software components are released under [OSI approved licenses](https://opensource.org/licenses).

---

# <!-- fit --> Demo Time

---


# GPU stacks

- ‚õî NVIDIA CUDA has a proprietary license
- ‚úÖ AMD ROCm stack is MIT licensed
  - `amdgpu` driver in kernel mainline

---

# Components for Semantic Search

- Encode semantics ‚ñ∂ **Embeddings**
- Find semantically similar objects ‚ñ∂ **Vector Database**

---

# Embedding Models

|                                                                                   | Dims | OSI License   | Pre Train Data | Fine Tune data |
| --------------------------------------------------------------------------------- | ---- | ------------- | -------------- | -------------- |
| [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) | 384  | ‚úÖ Apache-2.0 | ‚úÖ             | ‚úÖ             |
| [nomic-embed-text-v1](https://huggingface.co/nomic-ai/nomic-embed-text-v1)        | 768  | ‚úÖ Apache-2.0 | ‚úÖ             | ‚úÖ             |
| [bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)                | 1024 | ‚úÖ MIT        | ‚õî             | ‚õî             |
| [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1) | 1024 | ‚úÖ Apache-2.0 | ‚õî             | ‚õî             |

<!-- footer: "Find more embedding models here: https://huggingface.co/spaces/mteb/leaderboard" -->

---

<!-- footer: "" -->

# Embedding Inference

## [ü¶ô Ollama](https://github.com/ollama/ollama)

_Get up and running with large language models._

- Inference engine based on [llama.cpp](https://github.com/ggerganov/llama.cpp)
- Supports AMD GPU via ROCm
- CPU support (AVX, AVX2, AVX512, Apple Silicon)
- Quantization
- Model Library
- One model, one inference at a time

---

# Embedding Inference

|                                                                                          | OSI License   | ROCm support | Production |
| ---------------------------------------------------------------------------------------- | ------------- | ------------ | ---------- |
| [Ollama](https://github.com/ollama/ollama)                                               | ‚úÖ MIT        | ‚úÖ           | ‚õî         |
| [llama.cpp](https://github.com/ggerganov/llama.cpp)                                      | ‚úÖ MIT        | ‚úÖ           | ‚õî         |
| [HF Text Embeddings Interface](https://github.com/huggingface/text-embeddings-inference) | ‚úÖ Apache-2.0 | ‚õî           | ‚úÖ         |
| [Infinity](https://github.com/michaelfeil/infinity)                                      | ‚úÖ MIT        | ‚úÖ           | ‚úÖ         |

---

# Embedding Implementation

#### Start ollama

```sh
$ ollama serve &
$ ollama pull nomic-embed-text-v1
```

#### Generate embedding

```python
import ollama # pip install ollama

res = ollama.embeddings(
    model="nomic-embed-text-v1",
    prompt="This string")

res["embedding"] # [0.33, 0.62, 0.19, ...]
```

<!-- footer: "More code: https://github.com/rti/barebone-rag/blob/main/models.py" -->

---

# Vector Database

üêò Postgres can do it üéâ

## [pgvecto.rs](https://github.com/tensorchord/pgvecto.rs)

_Scalable, Low-latency and Hybrid-enabled Vector Search in Postgres._

A PostgreSQL extension written in Rust.

<!-- TODO: what is hybrid-enabled https://pgvecto-rs-docs-git-fork-gaocegege-hybrid-tensorchord.vercel.app/use-cases/hybrid-search.html -->
<!-- footer: "" -->

---

# Vector Database Licensing

|             | OSI License           |
| ----------- | --------------------- |
| PostgreSQL  | ‚úÖ PostgreSQL License |
| pgvector.rs | ‚úÖ Apache-2.0         |

<!-- footer: "Find more options here: https://ann-benchmarks.com/" -->

---

# Vector Database Implementation

#### Create PostgreSQL table using pgvecto.rs

```sql
CREATE EXTENSION vectors;

CREATE TABLE chunks (
  text TEXT NOT NULL,
  embedding VECTOR( 768 ) NOT NULL
);
```

#### Find most similar chunks

```sql
SELECT text FROM chunks
  ORDER BY embedding <-> [0.33, 0.62, 0.19, ...]
  LIMIT 5;
```

<!-- footer: "More code: https://github.com/rti/barebone-rag/blob/main/postgres.py" -->

---

# <!-- fit --> Components for Retrieval Augmented Generation (RAG)

- Find matching sources ‚ñ∂ **Semantic Search**
- Generate Response ‚ñ∂ **Large Language Model (LLM) Inference**

<!-- footer: "" -->

---

# LLM Inference

## [ü¶ô Ollama](https://github.com/ollama/ollama)

- LLMs too üòÄ Actually its core use case
- Great [model library](https://ollama.com/library) üìö

---

# LLM Inference

|                                                                                          | OSI License   | ROCm Support | Production |
| ---------------------------------------------------------------------------------------- | ------------- | ------------ | ---------- |
| [Ollama](https://github.com/ollama/ollama)                                               | ‚úÖ MIT        | ‚úÖ           | ‚õî         |
| [llama.cpp](https://github.com/ggerganov/llama.cpp)                                      | ‚úÖ MIT        | ‚úÖ           | ‚õî         |
| [vllm](https://github.com/vllm-project/vllm)                                             | ‚úÖ Apache-2.0 | ‚úÖ           | ‚úÖ         |
| [HF Text Generation Interface](https://github.com/huggingface/text-generation-inference) | ‚úÖ Apache-2.0 | ‚úÖ           | ‚úÖ         |

---

# LLM Inference Implementation

Generate a text based on a prompt

```python
import ollama # pip install ollama

res = ollama.chat(
    model="zephyr:7b-beta",
    messages=[{"role": "user", "content": f"Summarize this text: {text}"}],
    stream=False,
)
res["message"]["content"] # "The given text..."
```

<!-- footer: "More code: https://github.com/rti/barebone-rag/blob/main/models.py" -->

---

# Large Language Model Building Blocks

- Weights (binary)
- Pre training (source)
- Fine tuning data (source)
- Training code (build scripts)

---

# OSI - Open Source AI Initiative

- Intends to define Open Source models
- Defines which parts need to have [OSD](https://opensource.org/osd)-compliant licenses
- Draft, Release planned for October 2024
- [Latest draft (April 2024)](https://opensource.org/deepdive/drafts/the-open-source-ai-definition-draft-v-0-0-8)
  - Marks training data sets as optional
  - But requires data characteristics, labeling procedures, etc.

---

# LLMs with Openly Licensed Weights

|                                                                                      | OSI Weights   | PT Data | FT Data | Code |
| ------------------------------------------------------------------------------------ | ------------- | ------- | ------- | ---- |
| [Mistral 0.2 7b](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)          | ‚úÖ Apache-2.0 | ‚õî      | ‚õî      | ‚õî   |
| [HF Zephyr 7b beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)             | ‚úÖ MIT        | ‚õî      | ‚úÖ      | ‚úÖ   |
| [Microsoft Phi-3 Mini 3.8b](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) | ‚úÖ MIT        | ‚õî      | ‚õî      | ‚õî   |
| [Apple ELM 3b](https://huggingface.co/apple/OpenELM)                                 | ‚õî‚ùìASCL      | ‚úÖ      | ‚úÖ      | ‚úÖ   |
| [Meta Llama 3 8b](https://huggingface.co/meta-llama/Meta-Llama-3-8B)                 | ‚õî Custom     | ‚õî      | ‚õî      | ‚õî   |
| [Google Gemma 1.1 7b](https://huggingface.co/google/gemma-1.1-7b-it)                 | ‚õî Custom     | ‚õî      | ‚õî      | ‚õî   |

_PT: pre-training - FT: fine tuning_

<!-- footer: "Find more LLMs here: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" -->

---

# Open Source LLM Projects

- [Bloom](<https://en.wikipedia.org/wiki/BLOOM_(language_model)>) (2022), BigScience RAIL License v1.0, not SOTA, not OSD
- [Allan AI OLMo](https://huggingface.co/allenai/OLMo-7B) based on the [Dolma dataset](https://huggingface.co/datasets/allenai/dolma)
- [LumiOpen Viking](https://huggingface.co/LumiOpen/Viking-7B) built on [Lumi Supercomputer](https://www.lumi-supercomputer.eu/)
- [HuggingFace StarChat2](https://huggingface.co/HuggingFaceH4/starchat2-15b-v0.1) focussed on code
- [OpenGPT-X](https://opengpt-x.de/en/project/) EU funded

---

# Conclusion

- ‚úÖ Almost all software components are available with OSI approved licenses
- ‚úÖ ROCm works and people are using it
- ‚ùì Definition of open source models unclear
- ü§î Identifying truly open source models is complicated
- ‚è≥ Interesting developments ongoing

<!-- footer: "" -->
